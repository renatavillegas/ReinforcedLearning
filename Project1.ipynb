{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf12f58b-cf85-445d-837a-703fef223d96",
   "metadata": {},
   "source": [
    "# MO436A Project1: Reinforced learning algorithms evaluation\n",
    "\n",
    "# 1. Problem description \n",
    "\n",
    "**Motivation** \n",
    "Model an agent to operate in a trading environment. \n",
    "\n",
    "**Objectve** \n",
    "The agent objective is to maximize the gain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348c9ca-502d-4aa2-97f4-74f78fa79f12",
   "metadata": {},
   "source": [
    "# 2. Environments \n",
    "\n",
    "## 2.1 Stocrastic\n",
    "\n",
    "The trading problem is pretty stocrastic, so, to simplify the model, the prices are configurated with a normal distribution with a \n",
    "configurable mean and volatility. \n",
    "\n",
    "This is an example of pricing data generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e29ab-e703-4d03-95d8-a8da501f346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_return = 0.001  # Average hourly return (negative = downward trend)\n",
    "volatility = 0.003    # Standard deviation of returns (3% volatility)\n",
    "\n",
    "def generate_intraday_prices(num_days=100, hours_per_day=10, start_price=10):\n",
    "    \"\"\"\n",
    "    Generate intraday price data using geometric random walk.\n",
    "    \n",
    "    Args:\n",
    "        num_days: Number of trading days to generate\n",
    "        hours_per_day: Number of hourly steps per day\n",
    "        start_price: Initial price for all days\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of shape (num_days, hours_per_day) with price values\n",
    "    \"\"\"\n",
    "    prices = []\n",
    "    for _ in range(num_days):\n",
    "        returns = np.random.normal(loc=mean_return, scale=volatility, size=hours_per_day)\n",
    "        day_prices = start_price * np.exp(np.cumsum(returns))\n",
    "        prices.append(day_prices)\n",
    "    plot_prices(np.array(prices), num_days)\n",
    "    return np.array(prices)\n",
    "\n",
    "\n",
    "def plot_prices(prices, num_days):\n",
    "    \"\"\"\n",
    "    Plot daily price variations (percentage and absolute).\n",
    "    \n",
    "    Args:\n",
    "        prices: 2D array of shape (num_days, hours_per_day)\n",
    "        num_days: Number of days for labeling\n",
    "    \"\"\"\n",
    "    # Calculate daily variations\n",
    "    # Absolute change (close - open) and percentage change\n",
    "    open_prices = prices[:, 0]\n",
    "    close_prices = prices[:, -1]\n",
    "    daily_abs_change = close_prices - open_prices\n",
    "    daily_pct_change = (close_prices / open_prices - 1) * 100\n",
    "\n",
    "    # Figure 1: Daily percentage price variation\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    ax.bar(np.arange(1, num_days+1), daily_pct_change, color=['#2ca02c' if x>=0 else '#d62728' for x in daily_pct_change], width=0.8)\n",
    "\n",
    "    ax.set_title('Daily Percentage Price Variation (Close vs. Open)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Day', fontsize=4)\n",
    "    ax.set_ylabel('Variation (%)', fontsize=4)\n",
    "\n",
    "    # Horizontal line at 0%\n",
    "    ax.axhline(0, color='black', linewidth=1)\n",
    "\n",
    "    # Better x-axis labels (mark every 5 days)\n",
    "    ax.set_xticks(np.arange(1, num_days+1, 5))\n",
    "\n",
    "    # Text with basic statistics\n",
    "    mean_change = np.mean(daily_pct_change)\n",
    "    std_change = np.std(daily_pct_change)\n",
    "    ax.text(0.99, 0.02, f'Mean: {mean_change:.2f}%\\nStd Dev: {std_change:.2f}%', transform=ax.transAxes,\n",
    "            ha='right', va='bottom', bbox=dict(boxstyle='round', fc='white', ec='gray', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Figure 2: Daily absolute price variation\n",
    "    fig2, ax2 = plt.subplots(figsize=(6, 2))\n",
    "    ax2.plot(np.arange(1, num_days+1), daily_abs_change, marker='o', linewidth=1.5, color='#1f77b4')\n",
    "    ax2.set_title('Daily Absolute Price Variation (Close - Open)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Day', fontsize=4)\n",
    "    ax2.set_ylabel('Î” Price', fontsize=4)\n",
    "    ax2.axhline(0, color='black', linewidth=1)\n",
    "    ax2.set_xticks(np.arange(1, num_days+1, 5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5513b-f9df-4ac1-8ec2-1b694cbc63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = generate_intraday_prices(num_days=15, hours_per_day=10, start_price=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738737ee-47a3-4bde-80fd-6aea6a544442",
   "metadata": {},
   "source": [
    "## 2.2 Deterministic environment\n",
    "\n",
    "In this case, the deterministic events means the system might know, given the current price of the stock, what will be the price of the stock in the next hour (next step). The The Rulkov Map is used to generate price movements deterministically, since it can create a \"caotic\" price evolution, mantaining. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1551cc2a-1357-4cb4-ab07-d1a843064d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DeterministicTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Deterministic trading environment using the Rulkov Map for price generation.\n",
    "    \n",
    "    The Rulkov Map is a chaotic dynamical system that generates deterministic,\n",
    "    reproducible price movements without randomness.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, n_steps=10, start_price=10.0,\n",
    "                 alpha=4.0, beta=10.0, sigma=0.01, mu=0.001,\n",
    "                 window_size=3):\n",
    "        \"\"\"\n",
    "        Initialize the deterministic trading environment.\n",
    "        \n",
    "        Args:\n",
    "            n_steps: Number of steps per episode (trading hours)\n",
    "            start_price: Initial asset price\n",
    "            alpha, beta, sigma, mu: Rulkov Map parameters\n",
    "            window_size: Number of historical prices to include in observation\n",
    "        \"\"\"\n",
    "        super(DeterministicTradingEnv, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.start_price = start_price\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu\n",
    "        self.window_size = window_size\n",
    "        self.initial_cash = 100\n",
    "        self.cash = self.initial_cash\n",
    "\n",
    "        # Action and observation spaces\n",
    "        self.action_space = spaces.Discrete(3)  # Hold (0), Buy (1), Sell (2)\n",
    "        # State: last N prices + cash + asset holdings\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=np.inf, shape=(self.window_size + 2,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def f(self, x, y, alpha):\n",
    "        \"\"\"Rulkov Map function.\"\"\"\n",
    "        if x <= 0:\n",
    "            return alpha / (1 - x) + y\n",
    "        elif 0 < x < (alpha + y):\n",
    "            return alpha + y\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def rulkov_map(self, x, y):\n",
    "        \"\"\"Update state using Rulkov Map dynamics.\"\"\"\n",
    "        x_next = self.f(x, y + self.beta, self.alpha)\n",
    "        y_next = y - self.mu * (x_next + 1) + self.mu * self.sigma\n",
    "        return x_next, y_next\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.x = -1.0\n",
    "        self.y = -3.5\n",
    "        self.t = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.asset = 0.0\n",
    "        self.price = self.start_price\n",
    "        # Initial price history\n",
    "        self.price_history = [self.price] * self.window_size\n",
    "        state = np.array(self.price_history + [self.cash, self.asset], dtype=np.float32)\n",
    "        return state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action: 0 = Hold, 1 = Buy, 2 = Sell\n",
    "            \n",
    "        Returns:\n",
    "            observation, reward, done, truncated, info\n",
    "        \"\"\"\n",
    "        # Update price via Rulkov Map\n",
    "        self.x, self.y = self.rulkov_map(self.x, self.y)\n",
    "        self.price *= np.exp(self.x * 0.001)\n",
    "\n",
    "        # Update price history\n",
    "        self.price_history.append(self.price)\n",
    "        if len(self.price_history) > self.window_size:\n",
    "            self.price_history.pop(0)\n",
    "\n",
    "        # Execute action\n",
    "        if action == 1 and self.cash > 0:  # Buy\n",
    "            self.asset += self.cash / self.price\n",
    "            self.cash = 0\n",
    "        elif action == 2 and self.asset > 0:  # Sell\n",
    "            self.cash += self.asset * self.price\n",
    "            self.asset = 0\n",
    "\n",
    "        # Calculate reward: portfolio value change\n",
    "        portfolio_value = self.cash + self.asset * self.price\n",
    "        reward = portfolio_value - self.initial_cash\n",
    "\n",
    "        # Advance time\n",
    "        self.t += 1\n",
    "        done = self.t >= self.n_steps\n",
    "\n",
    "        state = np.array(self.price_history + [self.cash, self.asset], dtype=np.float32)\n",
    "        return state, reward, done, False, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Render the current environment state.\"\"\"\n",
    "        print(f\"Step {self.t}: Price={self.price:.2f}, Cash={self.cash:.2f}, Asset={self.asset:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8918d9-149e-4931-a5f8-9ccdeaf57199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Price=10.09, Cash=100.00, Asset=0.00\n",
      "Step 2: Price=10.19, Cash=0.00, Asset=9.81\n",
      "Step 3: Price=10.18, Cash=99.90, Asset=0.00\n",
      "Step 4: Price=10.27, Cash=0.00, Asset=9.73\n",
      "Step 5: Price=10.38, Cash=100.95, Asset=0.00\n",
      "Step 6: Price=10.37, Cash=0.00, Asset=9.74\n",
      "Step 7: Price=10.45, Cash=0.00, Asset=9.74\n",
      "Step 8: Price=10.56, Cash=0.00, Asset=9.74\n",
      "Step 9: Price=10.55, Cash=102.78, Asset=0.00\n",
      "Step 10: Price=10.64, Cash=102.78, Asset=0.00\n"
     ]
    }
   ],
   "source": [
    "env = DeterministicTradingEnv()\n",
    "state, _ = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db9644-4b8f-4167-83dc-63ce965aa5a8",
   "metadata": {},
   "source": [
    "# 3. MDP Formulation\n",
    "\n",
    "**States**\n",
    " In this problem, the state is defined as: \n",
    " - Current cash\n",
    " - Current asset holdings\n",
    " - Last window_size prices\n",
    "The original problem is partially observable, as if only the current price is considered, it is not possible to know if is in an upward or downward trend, so a window_size is needed to make it oberservable.\n",
    "**Actions**\n",
    " The actions in this case are discrete:\n",
    "- 0 : Hold\n",
    "- 1 : Buy\n",
    "- 2 : Sell\n",
    "**Transactions**\n",
    "\n",
    "**Rewards**\n",
    "\n",
    "**Terminal Condition**\n",
    "- Rewards: The reward is defined by profit/loss of each action\n",
    "- Discount(gamma): long-term gains. \n",
    "\n",
    "**Simplfications in the model** \n",
    "- The prices are updated hourly and are updated 10 times a day. \n",
    "- The prices start with the same value each day.\n",
    "- The prices std_dev is constant. \n",
    "- Here we sell/buy all stocks in portifolium, which means it is the same as considering only one stock.\n",
    "\n",
    "**Environment characterists**\n",
    "- Episodic - As each day is treated as a complete episode. \n",
    "- Terminal States - There are not terminal states, but we could set one, for example, choose to stop at a maximum profit or maximum lose. \n",
    "- The states are continuous, as they are defined as the price.\n",
    "- The environment is stocrastic as the price vary ramdonly with a normal distribution.\n",
    "- This envrionment is partially observable, as we have information of only a window size of prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1690b9a-4abd-42ee-9813-5e7084e1f35a",
   "metadata": {},
   "source": [
    "**Transactions**\n",
    "In the determimnistc environment, the price is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f02e6-8204-4f61-9e7d-2aed777604f7",
   "metadata": {},
   "source": [
    "# 4. Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410d0c3-2705-442a-a4f1-c8a1f751a383",
   "metadata": {},
   "source": [
    "# 5. SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c51988-0299-4f4e-924b-82bc90b44854",
   "metadata": {},
   "source": [
    "# 6.Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60fd4fc-7935-40be-a607-277b499b23bf",
   "metadata": {},
   "source": [
    "# 7. Linear Function Approximator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e19cb2-6cfa-4439-b9c3-79b44a5ed704",
   "metadata": {},
   "source": [
    "# 8. DQN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
